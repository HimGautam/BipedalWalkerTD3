{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0371f",
   "metadata": {
    "id": "33c0371f",
    "outputId": "f6d4d74a-a6fe-4037-8adb-abf46578381d"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bfb28d",
   "metadata": {
    "id": "92bfb28d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from collections import deque\n",
    "import gym\n",
    "import os\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081d872",
   "metadata": {
    "id": "a081d872",
    "outputId": "16044a11-4001-4164-dda5-5c8fc10e8a31"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\")\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.shape[0]\n",
    "input_size, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db564ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordVideo(env, \"videos\", step_trigger=lambda t: t%100==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3376438",
   "metadata": {
    "id": "3dfa836e"
   },
   "outputs": [],
   "source": [
    "tau = 0.005             #soft target update\n",
    "gamma = 0.99             #discount factor\n",
    "\n",
    "\n",
    "actor_lr = 1e-3      \n",
    "critic_lr = 1e-3\n",
    "\n",
    "memory_size = int(1e6)\n",
    "minibatch_size = 128\n",
    "\n",
    "num_episodes = 2_000   #max episodes for training\n",
    "num_steps=800          #max number of steps in an episode\n",
    "\n",
    "noise_std = 0.2       #stddev of noise added for exploration\n",
    "start_after = 1000     #pure exploration before 1000 steps\n",
    "update_after = 1000    #start learning after 1000 steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49fd81d",
   "metadata": {
    "id": "b49fd81d",
    "outputId": "921ca797-15f0-4e7f-c394-73a420826a75",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# rm -rf ./logs/train_PPO/\n",
    "train_log_dir = \"logs/train_DDPG/\" + current_time\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {train_log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f73fa",
   "metadata": {
    "id": "ce2f73fa"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, input_size, output_size, size):\n",
    "        self.states = np.zeros(shape = (size, input_size), dtype=np.float32)\n",
    "        self.next_states = np.zeros(shape = (size, input_size), dtype=np.float32)\n",
    "        self.actions = np.zeros(shape = (size, output_size), dtype=np.float32)\n",
    "        self.rewards = np.zeros(shape = size, dtype=np.float32)\n",
    "        self.dones = np.zeros(shape = size, dtype=np.float32)\n",
    "        self.pointer=0\n",
    "        self.size=0\n",
    "        self.max_size=size\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.states[self.pointer] = state\n",
    "        self.next_states[self.pointer] = next_state\n",
    "        self.actions[self.pointer] = action\n",
    "        self.rewards[self.pointer] =  reward\n",
    "        self.dones[self.pointer] = done\n",
    "        self.pointer = (self.pointer + 1)%self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "        \n",
    "    def sample(self, minibatch_size = 32):\n",
    "        idxs = np.random.randint(0, self.size, size=minibatch_size)\n",
    "        \n",
    "        return self.states[idxs],\\\n",
    "               self.actions[idxs],\\\n",
    "               self.rewards[idxs],\\\n",
    "               self.next_states[idxs],\\\n",
    "               self.dones[idxs]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6894338c",
   "metadata": {
    "id": "6894338c"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic_1 = self.build_critic()\n",
    "        self.critic_2 = self.build_critic()\n",
    "        \n",
    "        self.target_actor = copy(self.actor)\n",
    "        self.target_critic_1 = copy(self.critic_1)\n",
    "        self.target_critic_2 = copy(self.critic_2)\n",
    "        \n",
    "        self.actor_optimizer = Adam(lr=actor_lr)\n",
    "        self.critic_optimizer =  Adam(lr=critic_lr)\n",
    "        \n",
    "        self.memory = ReplayMemory(input_size, output_size, memory_size)\n",
    "        self.learn_count = 0\n",
    "        self.noise_std = noise_std\n",
    "        self.actor_update_itr = 2\n",
    " \n",
    "    def build_actor(self):\n",
    "        init = tf.random_uniform_initializer(minval=-3e-3, maxval=3e-3)\n",
    "        inputs = Input(shape = input_size)\n",
    "        x = Dense(400, activation = 'relu')(inputs)\n",
    "        x = Dense(300, activation = 'relu')(x)\n",
    "        x = Dense(output_size, activation = \"tanh\", kernel_initializer=init)(x)\n",
    "        model = Model(inputs = inputs, outputs = x)\n",
    "        return model\n",
    "    \n",
    "    def build_critic(self):\n",
    "        init = tf.random_uniform_initializer(minval=-3e-3, maxval=3e-3)\n",
    "        \n",
    "        input1 = Input(shape = input_size)\n",
    "        input2 = Input(shape = output_size)\n",
    "        x = Dense(400, activation=\"relu\")(tf.concat([input1, input2], axis =1))\n",
    "        x = Dense(300, activation=\"relu\")(x)\n",
    "        x = Dense(1, activation = 'linear', kernel_initializer=init)(x)\n",
    "        model = Model(inputs = [input1, input2], outputs = x)\n",
    "        return model\n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "        \n",
    "    @tf.function \n",
    "    def act(self, state, test = False):\n",
    "        act_value = self.actor(state)\n",
    "        if not test:\n",
    "            act_value += tf.random.normal(shape = act_value.shape, mean=0.0, stddev = self.noise_std)\n",
    "        return tf.clip_by_value(act_value, -1, 1)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def critic_learn(self, states, actions, rewards, next_states, dones):\n",
    "        rewards= tf.expand_dims(rewards, axis=1)\n",
    "        dones = tf.expand_dims(dones, axis=1)\n",
    "      \n",
    "        next_actions = self.target_actor(next_states)\n",
    "        next_actions += tf.clip_by_value(tf.random.normal(shape = next_actions.shape, mean = 0.0, stddev = 0.2), -0.5, 0.5)\n",
    "        next_actions = tf.clip_by_value(next_actions, -1, 1)\n",
    "        \n",
    "        next_q_1 = self.target_critic_1([next_states, next_actions])\n",
    "       \n",
    "        next_q_2 = self.target_critic_2([next_states, next_actions]) \n",
    "      \n",
    "        next_values = tf.math.minimum(next_q_1, next_q_2)\n",
    "      \n",
    "        target_q_values = rewards + gamma * (1-dones) * next_values\n",
    "      \n",
    "        with tf.GradientTape(persistent=True) as tape:  \n",
    "            \n",
    "              \n",
    "            pred_q_1 = self.critic_1([states, actions])\n",
    "            pred_q_2 = self.critic_2([states, actions])\n",
    "            \n",
    "            critic_loss = tf.reduce_mean((target_q_values - pred_q_1)**2) + tf.reduce_mean((target_q_values - pred_q_2)**2)\n",
    "            \n",
    "        critic_1_grads = tape.gradient(critic_loss, self.critic_1.trainable_weights)\n",
    "        critic_2_grads = tape.gradient(critic_loss, self.critic_2.trainable_weights)\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(zip(critic_1_grads, self.critic_1.trainable_weights))\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_2_grads, self.critic_2.trainable_weights))\n",
    "        \n",
    "\n",
    "    @tf.function\n",
    "    def actor_learn(self, states, actions, rewards, next_states, dones):\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions_pred = self.actor(states)\n",
    "            actor_loss = self.critic_1([states, actions_pred])\n",
    "            actor_loss = -tf.reduce_mean(actor_loss)\n",
    "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_weights)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_weights))\n",
    "    \n",
    "    def learn(self):\n",
    "        if agent.memory.size<minibatch_size:\n",
    "            return \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(minibatch_size)\n",
    "        self.critic_learn(states, actions, rewards, next_states, dones) \n",
    "        \n",
    "        self.learn_count += 1\n",
    "        \n",
    "        if self.learn_count%self.actor_update_itr != 0:\n",
    "            return\n",
    "       \n",
    "        self.learn_count = 0\n",
    "        \n",
    "        self.actor_learn(states, actions, rewards, next_states, dones)\n",
    "        self.update_target_weights()\n",
    "        \n",
    "    def update_target_weights(self):\n",
    "        target_actor_weights = self.target_actor.get_weights()\n",
    "        weights=[]\n",
    "        for i, weight in enumerate(self.actor.get_weights()):\n",
    "            weights.append(weight * tau + target_actor_weights[i] * (1-tau))\n",
    "        self.target_actor.set_weights(weights)\n",
    "        \n",
    "                           \n",
    "        target_critic_1_weights = self.target_critic_1.get_weights()\n",
    "        weights=[]\n",
    "        for i, weight in enumerate(self.critic_1.get_weights()):\n",
    "            weights.append(weight * tau + target_critic_1_weights[i] * (1-tau))\n",
    "        self.target_critic_1.set_weights(weights)\n",
    "        \n",
    "        target_critic_2_weights = self.target_critic_2.get_weights()\n",
    "        weights=[]\n",
    "        for i, weight in enumerate(self.critic_2.get_weights()):\n",
    "            weights.append(weight * tau + target_critic_2_weights[i] * (1-tau))\n",
    "        self.target_critic_2.set_weights(weights)\n",
    "                           \n",
    "\n",
    "\n",
    "    def load(self, name):\n",
    "        self.actor.load_weights(name + \"actor.hdf5\")\n",
    "        self.critic_1.load_weights(name + \"critic_1.hdf5\")\n",
    "        self.critic_2.load_weights(name + \"critic_2.hdf5\")\n",
    "        \n",
    "        self.target_actor.load_weights(name + \"actor.hdf5\")\n",
    "        self.target_critic_1.load_weights(name + \"critic_1.hdf5\")\n",
    "        self.target_critic_2.load_weights(name + \"critic_2.hdf5\")\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.actor.save_weights(name + \"actor.hdf5\")\n",
    "        self.critic_1.save_weights(name + \"critic_1.hdf5\") \n",
    "        self.critic_2.save_weights(name + \"critic_2.hdf5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcefa425",
   "metadata": {
    "id": "bcefa425"
   },
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767e128",
   "metadata": {
    "id": "d767e128",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training \n",
    "best_score = env.reward_range[0]\n",
    "score_history = deque(maxlen=100)\n",
    "global_step = 0\n",
    "for ep in range(0, num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    step = 0\n",
    "    for step in range(num_steps):\n",
    "        if global_step<start_after:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action =  agent.act(np.expand_dims(state, axis = 0))\n",
    "            action = np.squeeze(action)\n",
    "           \n",
    "        next_state, reward, done, _ =  env.step(action)\n",
    "        score += reward\n",
    "        global_step += 1\n",
    "    \n",
    "        agent.memory.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if global_step>update_after:\n",
    "            agent.learn()\n",
    "            \n",
    "        if done:\n",
    "            break\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar(\"Charts/score\", score, global_step)\n",
    "        tf.summary.scalar(\"Charts/episode_length\", step, global_step)\n",
    "        tf.summary.scalar(\"Charts/exploration\", noise_std, global_step)\n",
    "        \n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history)\n",
    "    print(f\"Episode: {ep}, Len: {step}, Score: {score}, Avg Score: {avg_score}, Global_Step: {global_step}\")  \n",
    "    if avg_score > best_score:\n",
    "        best_score = score\n",
    "        agent.save(\"DDPG_\") \n",
    "    if ep%25==0:\n",
    "        print(f\"Testing\")\n",
    "        for _ in range(2):\n",
    "            state=env.reset()\n",
    "            done = False\n",
    "            test_score=0\n",
    "            test_step=0\n",
    "            while not done:\n",
    "                action = agent.act(np.expand_dims(state, axis =0), test=True)\n",
    "                next_state, reward, done, _ = env.step(np.squeeze(action))\n",
    "                test_score+= reward\n",
    "                test_step+=1\n",
    "                state=next_state\n",
    "            print(f\"Episode ended with length {test_step} and a score of {test_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Pretrained Agent\n",
    "agent.load(\"Latest_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac527eaf",
   "metadata": {
    "id": "ac527eaf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing\n",
    "done = False\n",
    "state = env.reset()\n",
    "score=0\n",
    "step = 0\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = agent.act(np.expand_dims(state, axis=0), test=True)\n",
    "    next_state, reward, done, _ = env.step(np.squeeze(action))\n",
    "    score+= reward\n",
    "    state = next_state\n",
    "    print(action, reward)\n",
    "    step += 1\n",
    "print(f\"Episode ended in {step} steps with score of {score}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efdb1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6206ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "BipedalDDPG-Copy1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
